{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzWz2DPvpIsV","executionInfo":{"status":"ok","timestamp":1650960707499,"user_tz":-420,"elapsed":5684,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"d8812a80-5bac-43d8-a929-4bb4a22a55fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"AZyCuWn41yew"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2826,"status":"ok","timestamp":1650960712961,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"},"user_tz":-420},"id":"uVTYqzwGzxU-","outputId":"5b075351-2693-4466-b361-62d5880ff64d"},"outputs":[{"output_type":"stream","name":"stdout","text":["openjdk version \"1.8.0_312\"\n","OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n","OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"]}],"source":["# Install jdk8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# Set jdk environment path which enables you to run Pyspark in your Colab environment.\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n","!java -version"]},{"cell_type":"code","source":["\n","!pip install bigdl-dllib"],"metadata":{"id":"L46x21WY3OLI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650960716248,"user_tz":-420,"elapsed":3293,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"15b75012-e4f8-4955-d12d-4f38ae2f0bae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bigdl-dllib in /usr/local/lib/python3.7/dist-packages (2.0.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib) (1.15.0)\n","Requirement already satisfied: pyspark==2.4.6 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib) (2.4.6)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib) (1.21.6)\n","Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.7/dist-packages (from pyspark==2.4.6->bigdl-dllib) (0.10.7)\n"]}]},{"cell_type":"code","source":["!pip install -Uq emoji \\\n","                 optuna \\\n","                 flashtext \\\n","                 underthesea \\\n","                 scikit-learn \\\n","                 vncorenlp \\"],"metadata":{"id":"hhE1QeyVNEZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Library"],"metadata":{"id":"P_3oZ053NFhj"}},{"cell_type":"code","source":["# import necesary libraries and modules\n","\n","from bigdl.dllib.nn.layer import *\n","from bigdl.dllib.nn.criterion import *\n","from bigdl.dllib.utils.common import *\n","from bigdl.dllib.nnframes.nn_classifier import *\n","from bigdl.dllib.feature.common import *\n","import matplotlib.pyplot as plt\n","from __future__ import print_function\n","import os\n","import argparse\n","\n","\n","from bigdl.dllib.nncontext import *\n","init_nncontext()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlQ5oN1eJGhv","executionInfo":{"status":"ok","timestamp":1650960726704,"user_tz":-420,"elapsed":6342,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"1e0192ac-3f84-42e7-9356-015e9777e738"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.7/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-2.0.0-jar-with-dependencies.jar pyspark-shell \n"]},{"output_type":"execute_result","data":{"text/plain":["<SparkContext master=local[*] appName=pyspark-shell>"],"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://1b6b4b5618d2:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.4.6</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import glob\n","import pandas as pd\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import *\n","from pyspark.sql import functions as f\n","\n","from pyspark.sql.functions import col,length,trim\n","import re\n","from emoji import get_emoji_regexp\n","import unicodedata\n","from underthesea import word_tokenize\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n","from pyspark.ml.feature import StopWordsRemover, RegexTokenizer\n","from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, Word2Vec\n","from pyspark.ml import Pipeline"],"metadata":{"id":"nJF2Q3mUM_QD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[1]\") \\\n","                    .getOrCreate()"],"metadata":{"id":"ueqdqzyEMsct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObqIg1iz1xzX"},"source":["# Load dataset\n","/content/drive/MyDrive/DoAn/data/dataset.csv"]},{"cell_type":"code","source":["def load_data_spark(url = '/content/drive/MyDrive/DoAn/data/gannhan/'):\n","  folder= ['VA','BLong','Phuong','TH']\n","  address_files = []\n","  for i in folder:\n","    address_files += glob.glob(url+i+'/*.xlsx')\n","  temp_column = pd.read_excel(address_files[0]).columns.str.lower()\n","  sum_read_file=[]\n","  for i in address_files:\n","    temp_file = pd.read_excel(i)\n","    temp_file.columns= temp_column\n","  \n","    sum_read_file.append(temp_file)\n","  \n","  sum_read_file = pd.concat(sum_read_file)\n","  print(len(sum_read_file))\n","  return spark.createDataFrame(sum_read_file.astype(str))"],"metadata":{"id":"Z0f082X1laqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataDF = load_data_spark()"],"metadata":{"id":"Yr8mY8i7l0fQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650960734595,"user_tz":-420,"elapsed":5529,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"dba68bb9-a373-4e56-8c0e-95e5d121ac20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13018\n"]}]},{"cell_type":"markdown","metadata":{"id":"gXU5QFga6ZTt"},"source":["# Processing"]},{"cell_type":"code","source":["class Preporcessing:\n","  def __init__(self, basic_preprocessing = False, embedding_type = \"tfidf\", path_acronyms = None):\n","    self.basic_prepro = basic_preprocessing\n","    self.embedding_type = embedding_type\n","    if path_acronyms:\n","      self.dict_special = self.special_case(path_acronyms)\n","  \n","  def special_case(self, path_acronyms):\n","      special_w = pd.read_excel(path_acronyms)\n","      special_w = special_w.to_dict('index')\n","      dict_special={}\n","      for key, values in special_w.items():\n","        row = []\n","        for k,v in values.items():\n","          if len(v)>=3:\n","            row.append(v)\n","        if len(row) ==2:\n","          dict_special.update({row[1]:[row[0]]})\n","      return dict_special\n","\n","  def clean_text(self, text, special_w=None):\n","    # Unicode normalize\n","    text = unicodedata.normalize('NFC',text)\n","\n","    # Lower\n","    text = text.lower()\n","\n","    # Remove all emoji\n","    text = re.sub(get_emoji_regexp(),\"\",text)\n","\n","    #  Change #string to HASTAG \n","    if self.basic_prepro == False:\n","        text = re.sub('#\\S+',\"HASTAG\",text)\n","\n","        # # Find all price tag and change to GIÁ\n","        pricetag = '((?:(?:\\d+[,\\.]?)+) ?(?:nghìn đồng|đồng|k|vnd|d|đ))'\n","        text = re.sub(pricetag,\"PRICE\",text)\n","\n","        # Replace some special word\n","        replace_dct = {\"òa \":[\"oà \"], \"óa \":[\"oá \"], \"ỏa \":[\"oả \"], \"õa \":[\"oã \"], \"ọa \":[\"oạ \"],\n","                  \"òe\":[\"oè\"], \"óe\":[\"oé\"], \"ỏe\":[\"oẻ\"], \"õe\":[\"oẽ\"], \"ọe\":[\"oẹ\"],\n","                  \"ùy\":[\"uỳ\"], \"úy\":[\"uý\"], \"ủy\":[\"uỷ\"], \"ũy\":[\"uỹ\"], \"ụy\":[\"uỵ\"],\n","                  \"ùa\":[\"uà\"], \"úa \":[\"uá \"], \"ủa\":[\"uả\"], \"ũa\":[\"uã\"], \"ụa\":[\"uạ\"],\n","                  \"xảy\":[\"xẩy\"], \"bảy\":[\"bẩy\"], \"gãy\":[\"gẫy\"],\"nhân viên \":[\"nvien\"],\"quay\":['qay']}\n","        sum_special =  {**special_w, **replace_dct}    \n","        for key, values in sum_special.items():\n","          if type(values) == list:\n","            for v in values:\n","              text = text.replace(v, key)\n","        text = text.replace('ìnhh','ình')\n","\n","    # Remove all special char\n","    specialchar = r\"[\\\"#$%&'()*+,\\-\\/\\\\:;<=>@[\\]^_`{|}~\\n\\r\\t]\"\n","    text = re.sub(specialchar,\" \",text)\n","\n","    if self.basic_prepro == False:\n","        text = word_tokenize(text, format=\"text\")\n","\n","    return text\n","\n","  def clean_df(self, sparkDF):\n","    Clean_UDF = udf(lambda x: self.clean_text(x,self.dict_special),StringType())\n","    # Clean_Nan = udf (lambda x: label_encode[2] if x=='nan' else label_encode[int(float(x))],ArrayType(StringType()))\n","    Clean_Nan = udf (lambda x: float(-2.0) if x not in ['0.0','1.0','-1.0'] else float(x), FloatType())\n","    DF_Clean = sparkDF.select(Clean_UDF('cmt').alias(\"cmt\") , Clean_Nan('general').alias(\"general\"), Clean_Nan('price').alias(\"price\"), Clean_Nan('quality').alias(\"quality\"), Clean_Nan('service').alias(\"service\"), Clean_Nan('stylefood').alias(\"stylefood\"),Clean_Nan('location').alias(\"location\"), Clean_Nan('background').alias(\"background\"))\n","    return DF_Clean.withColumn(\"label\", f.array(\"general\",'price',\"quality\",\"service\",\"stylefood\",\"location\",\"background\").cast(ArrayType(FloatType())))\n","  \n","  def clean_sentenceDF(self, sentenceDF):\n","    Clean_UDF = udf(lambda x: self.clean_text(x,self.dict_special),StringType())\n","    DF_Clean = sentenceDF.select(Clean_UDF('cmt').alias(\"cmt\"))\n","    return DF_Clean\n","\n","  def split_data(self, sparkDF, train_ratio = 0.8, seed = 50):\n","    train_data, test_data = sparkDF.randomSplit([train_ratio, 1-train_ratio], seed)\n","    return train_data, test_data\n","\n","  def Embedding(self, num_feature):\n","    tokenizer = Tokenizer(inputCol=\"cmt\", outputCol=\"words\")\n","    newdb = VectorAssembler(inputCols=[\"features_vec\"], outputCol=\"features\")\n","\n","    if self.embedding_type == \"wordcount\":\n","      countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features_vec\", minDF=5, vocabSize=num_feature)\n","      pipeline = Pipeline(stages=[tokenizer,countVectors,newdb])\n","\n","    elif self.embedding_type == \"tfidf\":\n","      hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=num_feature)\n","      idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features_vec\" )\n","      pipeline = Pipeline(stages=[tokenizer,hashingTF,idf,newdb])\n","\n","    elif self.embedding_type == \"word2vec\":\n","      w2v = Word2Vec(vectorSize=num_feature, seed=42, inputCol=\"words\", outputCol=\"features_vec\")\n","      pipeline = Pipeline(stages=[tokenizer, w2v,newdb])\n","      \n","    else:\n","      raise ValueError(\"Embedding phải là 'wordcount', 'tfidf' hoặc 'word2vec'. Các embedding khác chưa hỗ trợ.\")\n","    \n","    return pipeline\n","def convertCase(float_num):\n","  \"\"\" so sánh với 0,-1,1,-2\"\"\" \n","  value_with_nan = abs(-2-float_num)\n","  value_with_neu = abs(0-float_num)\n","  value_with_neg = abs(-1-float_num)\n","  value_with_pos = abs(1-float_num)\n","  value_min = min([value_with_nan,value_with_neu,value_with_neg,value_with_pos])\n","  if value_min == value_with_nan:\n","    return -2.\n","  elif value_min== value_with_neu:\n","    return 0.\n","  elif value_min == value_with_neg:\n","    return -1.\n","  return 1.\n","\n","def edit_prediction_label(prediction_data):\n","  edit_pred_label = udf(lambda label_list: [convertCase(x) for x in label_list],ArrayType(FloatType()))\n","  list_pre = prediction_data.withColumn(\"prediction\",edit_pred_label('prediction'))\n","  return list_pre"],"metadata":{"id":"Xrbapy2Sm7PO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model:\n","  def __init__(self, model, input_dim, batch_size = 64,learning_rate = 0.2,epoch = 10,criterion=MSECriterion()):\n","    self.criterion=criterion\n","    self.model = model\n","    self.est =  NNEstimator(model, criterion, SeqToTensor([input_dim]), ArrayToTensor([7])) \\\n","                .setBatchSize(batch_size).setLearningRate(learning_rate).setMaxEpoch(epoch) \n","    self.NNmodel = None\n","  \n","  def train(self,train_data):\n","    self.NNmodel = self.est.fit(train_data)\n","\n","  def predict(self,test_data):\n","    return self.NNmodel.transform(test_data)\n","\n","  def evaluate(self, predict_data):\n","    res_final = edit_prediction_label(predict_data)\n","    list_pre = res_final.select('label','prediction').collect()\n","    acc_aspect = [0,0,0,0,0,0,0]\n","    total = len(list_pre)\n","    for i in range(0,len(list_pre)):\n","      for j in range(7):\n","        if list_pre[i][0][j] == list_pre[i][1][j]:\n","          acc_aspect[j] +=1\n","    return acc_aspect\n","\n","  def save_model(self, path_model):\n","    self.NNmodel.save(f\"{path_model}\")\n","    print(\"Save nnmodel successfull!\")\n","  \n","  def load_model(self, path):\n","    from bigdl.dllib.nnframes.nn_classifier import NNModel\n","    self.NNmodel = NNModel(self.model)\n","    self.NNmodel = self.NNmodel.load(path)\n","    print(\"Load nnmodel successfull!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kn-jmhBPOvf","executionInfo":{"status":"ok","timestamp":1650960735082,"user_tz":-420,"elapsed":497,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"1fe381a2-2b8f-4192-e821-124be8ca2a5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["creating: createMSECriterion\n"]}]},{"cell_type":"code","source":["def LSTM_model(input_size, hidden_size, output_size):\n","    model = Sequential()\n","    recurrent = Recurrent()\n","    recurrent.add(LSTM(input_size, hidden_size))\n","    model.add(InferReshape([-1, input_size], True))\n","    model.add(recurrent)\n","    model.add(Select(2, -1))\n","    model.add(Dropout(0.2))\n","    model.add(Linear(hidden_size, output_size))\n","    return model\n","def GRU_model(input_size, hidden_size, output_size):\n","    model = Sequential()\n","    recurrent = Recurrent()\n","    recurrent.add(GRU(input_size, hidden_size))\n","    model.add(InferReshape([-1, input_size], True))\n","    model.add(recurrent)\n","    model.add(Select(2, -1))\n","    model.add(Dropout(0.2))\n","    model.add(Linear(hidden_size, output_size))\n","    return model\n","def MLP(input_size, hidden_size, output_size):\n","    model = Sequential()\n","    model.add(Linear(input_size, hidden_size))\n","    model.add(ReLU())\n","    model.add(Linear(hidden_size, 256))\n","    model.add(ReLU())\n","    model.add(Linear(256, output_size))\n","    return model\n","\n","def RNN_model(input_size, hidden_size, output_size):\n","    model = Sequential()\n","    recurrent = Recurrent()\n","    recurrent.add(RnnCell(input_size, hidden_size, Tanh()))\n","    model.add(InferReshape([-1, input_size], True))\n","    model.add(recurrent)\n","    model.add(Select(2, -1))\n","    model.add(Dropout(0.2))\n","    model.add(Linear(hidden_size, output_size))\n","    return model\n"],"metadata":{"id":"_K9tt7J5Oh1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run and Save Model with embedding 300"],"metadata":{"id":"TZta3mnHt0oW"}},{"cell_type":"code","source":["def get_evalute(model,test_data):\n","  pred_data = model.predict(test_data)\n","  acc = model.evaluate(pred_data)\n","  aspect_name = ['general', 'price', 'quality', 'service', 'stylefood','location', 'background']\n","  total = len(pred_data.collect())\n","  for i in range(7):\n","    print('{} : {} '.format(aspect_name[i],acc[i]/total))\n","  print(\"Sum evaluate : \",sum(acc)/(total*7))\n","def get_embedding(embedding_name,data):\n","  preprocessing = Preporcessing(basic_preprocessing=False, embedding_type=embedding_name, path_acronyms='/content/drive/MyDrive/BigData/data/original/Acronyms.xlsx')\n","  sparkDF_cleaned = preprocessing.clean_df(dataDF)\n","  train_data, test_data = preprocessing.split_data(sparkDF_cleaned)\n","  embedding = preprocessing.Embedding(300)\n","  embedding_abc = embedding.fit(train_data)\n","  embedding_abc.save('/content/drive/MyDrive/DoAn/model/Embedding/{}'.format(embedding_name))\n","  train_data = embedding_abc.transform(train_data).select('features','label')\n","  test_data = embedding_abc.transform(test_data).select('features','label')\n","  return train_data,test_data\n","def train_model_and_save(dataDF):\n","  embedding_nameS = ['tfidf','wordcount','word2vec']\n","  model_train =[LSTM_model(300,256,7),GRU_model(300,256,7),MLP(300,256,7),RNN_model(300,256,7)]\n","  model_name = ['lstm','gru','mlp','rnn']\n","  for embedding_name in embedding_nameS:\n","    print('------------------------------------------Embedding : {}----------------------------------------'.format(embedding_name))\n","    train_data,test_data = get_embedding(embedding_name,dataDF)\n","    for j in range(4):\n","      print(model_name[j])\n","      model = Model(model_train[j], 300, epoch=10, batch_size = 64,learning_rate = 0.2, criterion= MSECriterion())\n","      model.train(train_data)\n","      model.save_model('/content/drive/MyDrive/DoAn/model/Model_BigDL/{}/{}'.format(model_name[j],embedding_name))\n","      get_evalute(model,test_data)\n","train_model_and_save(dataDF)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvqG2oqMuB9u","executionInfo":{"status":"ok","timestamp":1650946106768,"user_tz":-420,"elapsed":2876784,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"63fef467-23fe-46ba-ad19-452fc4141714"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createLSTM\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createGRU\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createSequential\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createRnnCell\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","------------------------------------------Embedding : tfidf----------------------------------------\n","lstm\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.46656976744186046 \n","price : 0.5508720930232558 \n","quality : 0.26405038759689925 \n","service : 0.561046511627907 \n","stylefood : 0.748546511627907 \n","location : 0.7170542635658915 \n","background : 0.5944767441860465 \n","Sum evaluate :  0.5575166112956811\n","gru\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.4258720930232558 \n","price : 0.5368217054263565 \n","quality : 0.2863372093023256 \n","service : 0.5232558139534884 \n","stylefood : 0.6918604651162791 \n","location : 0.7194767441860465 \n","background : 0.5770348837209303 \n","Sum evaluate :  0.5372369878183831\n","mlp\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.19864341085271317 \n","price : 0.13372093023255813 \n","quality : 0.40988372093023256 \n","service : 0.23594961240310078 \n","stylefood : 0.08042635658914729 \n","location : 0.05717054263565891 \n","background : 0.17829457364341086 \n","Sum evaluate :  0.18486987818383166\n","rnn\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.45106589147286824 \n","price : 0.5358527131782945 \n","quality : 0.26356589147286824 \n","service : 0.5174418604651163 \n","stylefood : 0.7659883720930233 \n","location : 0.6700581395348837 \n","background : 0.6492248062015504 \n","Sum evaluate :  0.5504568106312292\n","------------------------------------------Embedding : wordcount----------------------------------------\n","lstm\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.5 \n","price : 0.5998062015503876 \n","quality : 0.3028100775193798 \n","service : 0.6075581395348837 \n","stylefood : 0.7999031007751938 \n","location : 0.8086240310077519 \n","background : 0.6787790697674418 \n","Sum evaluate :  0.6139258028792912\n","gru\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.5329457364341085 \n","price : 0.5770348837209303 \n","quality : 0.32170542635658916 \n","service : 0.6031976744186046 \n","stylefood : 0.7194767441860465 \n","location : 0.7858527131782945 \n","background : 0.6942829457364341 \n","Sum evaluate :  0.6049280177187154\n","mlp\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.19864341085271317 \n","price : 0.13372093023255813 \n","quality : 0.40988372093023256 \n","service : 0.23594961240310078 \n","stylefood : 0.08042635658914729 \n","location : 0.05717054263565891 \n","background : 0.17829457364341086 \n","Sum evaluate :  0.18486987818383166\n","rnn\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.5300387596899225 \n","price : 0.5726744186046512 \n","quality : 0.2005813953488372 \n","service : 0.6061046511627907 \n","stylefood : 0.7669573643410853 \n","location : 0.7994186046511628 \n","background : 0.6647286821705426 \n","Sum evaluate :  0.591500553709856\n","------------------------------------------Embedding : word2vec----------------------------------------\n","lstm\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.2621124031007752 \n","price : 0.32606589147286824 \n","quality : 0.12645348837209303 \n","service : 0.25339147286821706 \n","stylefood : 0.8972868217054264 \n","location : 0.8493217054263565 \n","background : 0.40939922480620156 \n","Sum evaluate :  0.44629014396456257\n","gru\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.2868217054263566 \n","price : 0.38275193798449614 \n","quality : 0.15164728682170542 \n","service : 0.3638565891472868 \n","stylefood : 0.8929263565891473 \n","location : 0.8217054263565892 \n","background : 0.49079457364341084 \n","Sum evaluate :  0.4843576965669989\n","mlp\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.19864341085271317 \n","price : 0.13372093023255813 \n","quality : 0.40988372093023256 \n","service : 0.23594961240310078 \n","stylefood : 0.08042635658914729 \n","location : 0.05717054263565891 \n","background : 0.17829457364341086 \n","Sum evaluate :  0.18486987818383166\n","rnn\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.35707364341085274 \n","price : 0.37257751937984496 \n","quality : 0.17926356589147288 \n","service : 0.3280038759689923 \n","stylefood : 0.8905038759689923 \n","location : 0.8028100775193798 \n","background : 0.5169573643410853 \n","Sum evaluate :  0.49245570321151716\n"]}]},{"cell_type":"code","source":["preprocessing = Preporcessing(basic_preprocessing=False, embedding_type=\"tfidf\", path_acronyms='/content/drive/MyDrive/BigData/data/original/Acronyms.xlsx')\n","sparkDF_cleaned = preprocessing.clean_df(dataDF)\n","train_data, test_data = preprocessing.split_data(sparkDF_cleaned)\n","embedding = preprocessing.Embedding(300)\n"],"metadata":{"id":"jjsnzaBLonVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_abc = embedding.fit(train_data)"],"metadata":{"id":"7PzerWS-AaNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_abc.save('/content/drive/MyDrive/DoAn/model/Embedding/wordcount')"],"metadata":{"id":"vHq676L1Bi8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PZhyHvEhQjro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml import Pipeline, PipelineModel\n","embedding_test = PipelineModel.load('/content/drive/MyDrive/DoAn/model/Embedding/wordcount')"],"metadata":{"id":"dphcqC0dQXDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_data = embedding_abc.transform(train_data).select('features','label')\n","test_data = embedding_abc.transform(test_data).select('features','label')"],"metadata":{"id":"C25t4zpRqRoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run and Save Model with Custom Embedding \n"],"metadata":{"id":"9nu__DXBOgeb"}},{"cell_type":"code","source":["def get_evalute(model,test_data):\n","  pred_data = model.predict(test_data)\n","  acc = model.evaluate(pred_data)\n","  aspect_name = ['general', 'price', 'quality', 'service', 'stylefood','location', 'background']\n","  total = len(pred_data.collect())\n","  for i in range(7):\n","    print('{} : {} '.format(aspect_name[i],acc[i]/total))\n","  print(\"Sum evaluate : \",sum(acc)/(total*7))\n","def prepare_model(embedding_dim,embedtype,model_type):\n","  if model_type == \"lstm\":\n","    model = LSTM_model(embedding_dim, 256, 7)\n","  elif model_type == \"gru\":\n","    model = GRU_model(embedding_dim, 256, 7)\n","  elif model_type == \"mlp\":\n","    if embedtype == \"word2vec\":\n","      hidden_size = 256\n","    else:\n","      hidden_size = 1000\n","    model = MLP(embedding_dim, hidden_size, 7)\n","  elif model_type == \"rnn\":\n","    model = RNN_model(embedding_dim, 256, 7)\n","  return model\n","def get_embedding(embedding_name,data,embedding_dim):\n","  preprocessing = Preporcessing(basic_preprocessing=False, embedding_type=embedding_name, path_acronyms='/content/drive/MyDrive/BigData/data/original/Acronyms.xlsx')\n","  sparkDF_cleaned = preprocessing.clean_df(dataDF)\n","  train_data, test_data = preprocessing.split_data(sparkDF_cleaned)\n","  embedding = preprocessing.Embedding(embedding_dim)\n","  embedding_abc = embedding.fit(train_data)\n","  embedding_abc.save('/content/drive/MyDrive/DoAn/Model_Custom/Embedding/{}'.format(embedding_name))\n","  train_data = embedding_abc.transform(train_data).select('features','label')\n","  test_data = embedding_abc.transform(test_data).select('features','label')\n","  return train_data,test_data\n","def train_model_and_save2(dataDF):\n","  embedding_nameS = ['tfidf','wordcount','word2vec']\n","  dict_embedding = {'tfidf':3000,'wordcount':3000,'word2vec':300}\n","  model_name = ['lstm','gru','mlp','rnn']\n","  for embedding_name in embedding_nameS:\n","    print('------------------------------------------Embedding : {}----------------------------------------'.format(embedding_name))\n","    train_data,test_data = get_embedding(embedding_name,dataDF,dict_embedding[embedding_name])\n","    for j in range(4):\n","      print(model_name[j])\n","      model_train= prepare_model(dict_embedding[embedding_name],embedding_name,model_name[j])\n","      model = Model(model_train, dict_embedding[embedding_name], epoch=10, batch_size = 64,learning_rate = 0.2, criterion= MSECriterion())\n","      model.train(train_data)\n","      model.save_model('/content/drive/MyDrive/DoAn/Model_Custom/Model_BigDL/{}/{}'.format(model_name[j],embedding_name))\n","      get_evalute(model,test_data)\n","train_model_and_save2(dataDF)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxCCwE3R0kqK","executionInfo":{"status":"ok","timestamp":1650964199353,"user_tz":-420,"elapsed":3451888,"user":{"displayName":"Anh Tran Viet","userId":"07919309494707415174"}},"outputId":"586dc7ef-2745-4de1-f632-cd531c522eed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------Embedding : tfidf----------------------------------------\n","lstm\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createLSTM\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.6317204301075269 \n","price : 0.6693548387096774 \n","quality : 0.44086021505376344 \n","service : 0.6778033794162827 \n","stylefood : 0.8252688172043011 \n","location : 0.8137480798771122 \n","background : 0.7258064516129032 \n","Sum evaluate :  0.6835088874259381\n","gru\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createGRU\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.5898617511520737 \n","price : 0.6359447004608295 \n","quality : 0.4435483870967742 \n","service : 0.6655145929339478 \n","stylefood : 0.7903225806451613 \n","location : 0.8060675883256528 \n","background : 0.7223502304147466 \n","Sum evaluate :  0.6648014044327408\n","mlp\n","creating: createSequential\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.16551459293394777 \n","price : 0.09831029185867896 \n","quality : 0.3325652841781874 \n","service : 0.1816436251920123 \n","stylefood : 0.06720430107526881 \n","location : 0.039938556067588324 \n","background : 0.14861751152073732 \n","Sum evaluate :  0.14768488040377442\n","rnn\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createRnnCell\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.5848694316436251 \n","price : 0.6639784946236559 \n","quality : 0.4358678955453149 \n","service : 0.6113671274961597 \n","stylefood : 0.7795698924731183 \n","location : 0.7695852534562212 \n","background : 0.7465437788018433 \n","Sum evaluate :  0.6559688391485626\n","------------------------------------------Embedding : wordcount----------------------------------------\n","lstm\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createLSTM\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.5917818740399385 \n","price : 0.6766513056835638 \n","quality : 0.42780337941628266 \n","service : 0.6666666666666666 \n","stylefood : 0.8348694316436251 \n","location : 0.804915514592934 \n","background : 0.7580645161290323 \n","Sum evaluate :  0.6801075268817204\n","gru\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createGRU\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.6282642089093702 \n","price : 0.7123655913978495 \n","quality : 0.43509984639016897 \n","service : 0.6213517665130568 \n","stylefood : 0.8544546850998463 \n","location : 0.8667434715821812 \n","background : 0.7634408602150538 \n","Sum evaluate :  0.6973886328725039\n","mlp\n","creating: createSequential\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.7419354838709677 \n","price : 0.6766513056835638 \n","quality : 0.47235023041474655 \n","service : 0.5933179723502304 \n","stylefood : 0.8110599078341014 \n","location : 0.8617511520737328 \n","background : 0.706605222734255 \n","Sum evaluate :  0.6948101821373711\n","rnn\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createRnnCell\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.6417050691244239 \n","price : 0.6900921658986175 \n","quality : 0.40514592933947774 \n","service : 0.6486175115207373 \n","stylefood : 0.8072196620583717 \n","location : 0.8245007680491552 \n","background : 0.7811059907834101 \n","Sum evaluate :  0.6854838709677419\n","------------------------------------------Embedding : word2vec----------------------------------------\n","lstm\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createLSTM\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.3782642089093702 \n","price : 0.5737327188940092 \n","quality : 0.20890937019969277 \n","service : 0.38095238095238093 \n","stylefood : 0.9166666666666666 \n","location : 0.8978494623655914 \n","background : 0.4976958525345622 \n","Sum evaluate :  0.5505815229317533\n","gru\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createSigmoid\n","creating: createGRU\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.3794162826420891 \n","price : 0.42780337941628266 \n","quality : 0.24423963133640553 \n","service : 0.41551459293394777 \n","stylefood : 0.9105222734254992 \n","location : 0.8721198156682027 \n","background : 0.4857910906298003 \n","Sum evaluate :  0.5336295808646039\n","mlp\n","creating: createSequential\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createReLU\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.4001536098310292 \n","price : 0.40706605222734255 \n","quality : 0.282642089093702 \n","service : 0.3671274961597542 \n","stylefood : 0.9036098310291859 \n","location : 0.7104454685099847 \n","background : 0.4270353302611367 \n","Sum evaluate :  0.49972569673030504\n","rnn\n","creating: createSequential\n","creating: createRecurrent\n","creating: createTanh\n","creating: createRnnCell\n","creating: createInferReshape\n","creating: createSelect\n","creating: createDropout\n","creating: createLinear\n","creating: createMSECriterion\n","creating: createSeqToTensor\n","creating: createArrayToTensor\n","creating: createFeatureLabelPreprocessing\n","creating: createNNEstimator\n","creating: createToTuple\n","creating: createChainedPreprocessing\n","Save nnmodel successfull!\n","general : 0.4312596006144393 \n","price : 0.6052227342549923 \n","quality : 0.29531490015360984 \n","service : 0.4543010752688172 \n","stylefood : 0.9112903225806451 \n","location : 0.8690476190476191 \n","background : 0.5664362519201229 \n","Sum evaluate :  0.5904103576914637\n"]}]}]}